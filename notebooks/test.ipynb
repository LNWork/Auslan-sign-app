{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# test test test\n",
    "\n",
    "def check_list_contained(A, B):\n",
    "  # convert list A to string\n",
    "    A_str = ' '.join(map(str, A))\n",
    "    # convert list B to string\n",
    "    B_str = ' '.join(map(str, B))\n",
    "    # find all instances of A within B\n",
    "    instances = re.findall(A_str, B_str)\n",
    "\n",
    "    # return True if any instances were found, False otherwise\n",
    "    return len(instances) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE HERE TO IMPORT OR SAVE\n",
    "saved = False\n",
    "\n",
    "if saved:\n",
    "    data = np.load('processed_data.npz')\n",
    "    xData = data['xData']\n",
    "    yData = data['yData']\n",
    "else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_file' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Create a ProcessPoolExecutor for parallel processing\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Use a list comprehension to submit the processing of each file\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(process_file, file_name): file_name \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m files}\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m     80\u001b[0m         reshaped_data, label, maxFrames \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n",
      "Cell \u001b[0;32mIn[19], line 77\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Create a ProcessPoolExecutor for parallel processing\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Use a list comprehension to submit the processing of each file\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     futures \u001b[38;5;241m=\u001b[39m {\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m: file_name \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m files}\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m     80\u001b[0m         reshaped_data, label, maxFrames \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/process.py:715\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_lock:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken:\n\u001b[0;32m--> 715\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broken)\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_thread:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot schedule new futures after shutdown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing the JSON files\n",
    "\n",
    "# Load the DataFrames outside of the function\n",
    "isosignsDF = pd.read_excel('datasignlist.xlsx', sheet_name='IsoSigns', usecols=\"B:D\")\n",
    "singlewordDF = pd.read_excel('datasignlist.xlsx', sheet_name='SingleWord', usecols=\"B:D\")\n",
    "\n",
    "def get_gloss_label(file_name, isosignsDF, singlewordDF):\n",
    "    # Check if the file name starts with a digit to determine which DataFrame to use\n",
    "    # file_name = file_name.split(\"/\")[-1]\n",
    "    if file_name[0].isdigit():\n",
    "        # Extract Video_ID from the filename\n",
    "        video_id = file_name.split('_')[0]\n",
    "        # Retrieve the row from the isosigns DataFrame based on Video_ID\n",
    "        gloss_row = isosignsDF.loc[isosignsDF['Video_ID'] == int(video_id)]\n",
    "    else:\n",
    "        # Extract Video_Clip_Name from the filename\n",
    "        video_clip_name = '_'.join(file_name.split('_')[:3])\n",
    "        # Retrieve the row from the singleword DataFrame based on Video_Clip_Name\n",
    "        gloss_row = singlewordDF.loc[singlewordDF['Video_Clip_Name'] == video_clip_name]\n",
    "\n",
    "    # Check if a match was found and return the Gloss value\n",
    "    if not gloss_row.empty:\n",
    "        return gloss_row['Gloss'].values[0]  # Note the capitalization of 'Gloss'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_file(file):\n",
    "    print(f\"Processing {file}\")\n",
    "    largestFrames = 0\n",
    "    xData = []\n",
    "    yData = []\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        d = f.read()\n",
    "        vidData = json.loads(d)\n",
    "\n",
    "        data = []\n",
    "        for frame in vidData:\n",
    "            if 'data' not in frame.keys():\n",
    "                continue\n",
    "\n",
    "            fData = pd.DataFrame(frame['data'])\n",
    "            data.append(fData)\n",
    "\n",
    "        if len(data) == 0:\n",
    "            return None, None, largestFrames\n",
    "\n",
    "        combinedDF = pd.concat(data, ignore_index=True)\n",
    "        reshapedData = combinedDF.values.reshape((len(data), 75, 4))\n",
    "        largestFrames = max(largestFrames, len(data))\n",
    "\n",
    "        word = get_gloss_label(file, isosignsDF, singlewordDF)\n",
    "\n",
    "        # Handle missing gloss\n",
    "        if word is None:\n",
    "            print(f\"Warning: No gloss label found for {file}\")\n",
    "            return None, None, largestFrames\n",
    "\n",
    "        xData.append(reshapedData)\n",
    "        yData.append(word)\n",
    "\n",
    "    return xData, yData, largestFrames\n",
    "    \n",
    "files = glob.glob('json_keypoints/*.json')\n",
    "count = 0\n",
    "\n",
    "xData = []\n",
    "yData = []\n",
    "largestFrames = 0\n",
    "\n",
    "start = time.time()\n",
    "count = 0\n",
    "\n",
    "# Create a ProcessPoolExecutor for parallel processing\n",
    "with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "    # Use a list comprehension to submit the processing of each file\n",
    "    futures = {executor.submit(process_file, file_name): file_name for file_name in files}\n",
    "\n",
    "    for future in futures:\n",
    "        reshaped_data, label, maxFrames = future.result()\n",
    "        if reshaped_data is not None and label is not None:\n",
    "            xData.extend(reshaped_data)  # Extend instead of append to flatten the structure\n",
    "            yData.append(label)\n",
    "            largestFrames = max(largestFrames, maxFrames)\n",
    "\n",
    "        if count%500 == 0:\n",
    "            print(count)\n",
    "            print(time.time()-start)\n",
    "        count += 1\n",
    "\n",
    "xData = np.array(xData, dtype=object)\n",
    "yData = np.array(yData)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(xData, yData, test_size=0.2, stratify=yData, random_state=42)\n",
    "\n",
    "# print(\"Train samples:\", len(x_train), \"Test samples:\", len(x_test))\n",
    "print(\"Largest frames:\", largestFrames)\n",
    "print(\"Time taken:\", time.time() - start)\n",
    "\n",
    "np.savez_compressed('processed_data.npz', xData=xData, yData=yData)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
